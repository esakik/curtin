{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPg91hF1b1s6/wpOwMaNEkA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esakik/curtin/blob/main/reinforcement_learning/multi_armed_bandits_%CE%B5_greedy_optimism_non_stationary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PX1gqMJDLiJQ"
      },
      "outputs": [],
      "source": [
        "#######################################################################\n",
        "# Copyright (C)                                                       #\n",
        "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
        "# 2016 Tian Jun(tianjun.cpp@gmail.com)                                #\n",
        "# 2016 Artem Oboturov(oboturov@gmail.com)                             #\n",
        "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
        "# Permission given to modify the code as long as you keep this        #\n",
        "# declaration at the top                                              #\n",
        "#######################################################################\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MABEnv:\n",
        "\n",
        "    def __init__(self, n_arms=10, mean=0.0):\n",
        "        \"\"\"Initialize Multi-armed Bandits (MAB) Environment class.\n",
        "\n",
        "        Args:\n",
        "            n_arms: Number of arms to pull.\n",
        "            mean: Mean value for normal distributions.\n",
        "        \"\"\"\n",
        "        self.n_arms = n_arms\n",
        "        self.mean = mean\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset all the information.\"\"\"\n",
        "        # True reward drawn from the normal distribution with mean for each action\n",
        "        self.q_true = np.random.randn(self.n_arms) + self.mean\n",
        "\n",
        "        # Action index with best true reward\n",
        "        self.optimal_action = np.argmax(self.q_true)\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Take action and return reward.\n",
        "\n",
        "        Args:\n",
        "            action: Index of next action\n",
        "\n",
        "        Returns:\n",
        "            reward\n",
        "        \"\"\"\n",
        "        # Stochastic reward drawn from the normal distribution\n",
        "        return np.random.randn() + self.q_true[action]\n",
        "\n",
        "    def reward_distribution_example():\n",
        "        plt.violinplot(dataset=np.random.randn(200, 10) + np.random.randn(10))\n",
        "        plt.xlabel(\"Action\")\n",
        "        plt.ylabel(\"Reward distribution\")\n",
        "        plt.savefig('./reward_distribution_example.png')\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "kOScV1TpNAlw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonGreedyAgent:\n",
        "\n",
        "    def __init__(self, env, alpha=None, initial=0.5, epsilon=0.1):\n",
        "        \"\"\"Initialize ε-greedy Agent class.\n",
        "\n",
        "        Args:\n",
        "            env: Multi-armed Bandits (MAB) Environment class.\n",
        "            alpha: Step size parameter to value the latest estimation\n",
        "            initial: Optimistic initial values\n",
        "            epsilon: Probability for exploration.\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.initial = initial\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset all the information.\"\"\"\n",
        "        # Estimated reward for each action\n",
        "        self.q_estimation = np.zeros(self.env.n_arms) + self.initial\n",
        "\n",
        "        # Chosen times for each action\n",
        "        self.action_count = np.zeros(self.env.n_arms)\n",
        "\n",
        "    def policy(self):\n",
        "        \"\"\"Get index of next action.\n",
        "\n",
        "        Returns:\n",
        "            Index of next action\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(np.arange(self.env.n_arms))\n",
        "        return np.argmax(self.q_estimation)\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        \"\"\"Update estimation.\n",
        "\n",
        "        Args:\n",
        "            action: Index of next action\n",
        "            reward: Reward from environment\n",
        "        \"\"\"\n",
        "        if self.alpha is None:\n",
        "            # Q(A)←Q(A)+1/N(A)[R−Q(A)]\n",
        "            self.action_count[action] += 1\n",
        "            self.q_estimation[action] += 1.0 / self.action_count[action] * (reward - self.q_estimation[action])\n",
        "        else:\n",
        "            # Qn+1=Qn+α[Rn−Qn]\n",
        "            self.q_estimation[action] += self.alpha * (reward - self.q_estimation[action])"
      ],
      "metadata": {
        "id": "-dgIFMVeJT3k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Simulator:\n",
        "\n",
        "    def __init__(self, agents):\n",
        "        self.agents = agents\n",
        "\n",
        "    def run(self, runs=2000, steps=3000):\n",
        "        optimal_action_counts, rewards = self._run(self.agents, runs, steps)\n",
        "\n",
        "        plt.figure(figsize=(10, 20))\n",
        "\n",
        "        plt.subplot(2, 1, 1)\n",
        "        for eps, rewards in zip(epsilons, rewards):\n",
        "            plt.plot(rewards, label='epsilon = %.02f' % (eps))\n",
        "        plt.xlabel('steps')\n",
        "        plt.ylabel('average reward')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(2, 1, 2)\n",
        "        for eps, counts in zip(epsilons, optimal_action_counts):\n",
        "            plt.plot(counts, label='epsilon = %.02f' % (eps))\n",
        "        plt.xlabel('steps')\n",
        "        plt.ylabel('% optimal action')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.savefig('./simulation_result.png')\n",
        "        plt.close()\n",
        "\n",
        "    def _run(self, agents, runs, steps):\n",
        "        optimal_action_counts = np.zeros((len(agents), runs, steps))\n",
        "        rewards = np.zeros(optimal_action_counts.shape)\n",
        "\n",
        "        for i, agent in enumerate(agents):\n",
        "            for r in tqdm(range(runs)):\n",
        "                agent.env.reset()\n",
        "                agent.reset()\n",
        "\n",
        "                for s in range(steps):\n",
        "                    action = agent.policy()\n",
        "                    reward = agent.env.step(action=action)\n",
        "                    agent.update(action=action, reward=reward)\n",
        "\n",
        "                    rewards[i, r, s] = reward\n",
        "                    if action == agent.env.optimal_action:\n",
        "                        optimal_action_counts[i, r, s] = 1\n",
        "\n",
        "        optimal_action_counts = optimal_action_counts.mean(axis=1)\n",
        "        rewards = rewards.mean(axis=1)\n",
        "\n",
        "        return optimal_action_counts, rewards"
      ],
      "metadata": {
        "id": "h84SRhwh4vr7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = MABEnv(n_arms=10, mean=0.0)\n",
        "epsilons = [0, 0.1, 0.01, 0.5, 1.0]\n",
        "agents = [EpsilonGreedyAgent(env=env, epsilon=epsilon) for epsilon in epsilons]\n",
        "simulator = Simulator(agents=agents)\n",
        "simulator.run(runs=2000, steps=3000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkUXky3K7W2d",
        "outputId": "1a247dc6-f59b-4359-b70c-681310a120e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [00:40<00:00, 49.46it/s]\n",
            "100%|██████████| 2000/2000 [00:43<00:00, 45.64it/s]\n",
            "100%|██████████| 2000/2000 [00:35<00:00, 55.89it/s]\n",
            "100%|██████████| 2000/2000 [01:15<00:00, 26.50it/s]\n",
            "100%|██████████| 2000/2000 [01:47<00:00, 18.58it/s]\n"
          ]
        }
      ]
    }
  ]
}